{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8thnj-6HZwA7"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty #It is empty vector"
      ],
      "metadata": {
        "id": "wApmuG9UZ4oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(3) #It is 1-D vector"
      ],
      "metadata": {
        "id": "5VbfHmPMZ_m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(2, 3) #It is 2-D vector"
      ],
      "metadata": {
        "id": "Lx2Tl9KXaTTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(2, 2, 2, 2) # It is 4-D vector"
      ],
      "metadata": {
        "id": "qlCe5_WHat-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create tensor with random values:\n",
        "\n",
        "x = torch.rand(2, 2)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4kJEbexbFlP",
        "outputId": "6da16da9-7549-4ae9-e939-bea3827f555e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0406, 0.4780],\n",
            "        [0.0356, 0.2159]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create tensor with zero values or ones values\n",
        "\n",
        "x = torch.zeros(2, 2)\n",
        "print(x)\n",
        "y = torch.ones(2, 2)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONA-y1QXbRh3",
        "outputId": "4d700867-6435-4b55-9fbe-866729e39cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting datatype\n",
        "\n",
        "x = torch.ones(2, 2, dtype=torch.int)"
      ],
      "metadata": {
        "id": "JApsOvCJbtcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.size()) # So '.size()' function gives size of tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd3MSsSYb7-x",
        "outputId": "5e1a085e-04c8-4920-a298-159460b4f1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom values\n",
        "\n",
        "x = torch.tensor([2.5, 5.0, 7.5]) # 1-D tensor\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odUnzDgucBBg",
        "outputId": "ca90f2d0-0bf7-469b-9988-7626bb4353f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.5000, 5.0000, 7.5000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How tensor values and operations work:\n",
        "\n",
        "x = torch.tensor([[2, 3], [4, 5]]) # 2-D tensor\n",
        "y = torch.tensor([[8, 7], [6, 5]]) # 2-D tensor\n",
        "\n",
        "z1 = torch.add(x, y) # for addition, or we can also do as\n",
        "print(z1)\n",
        "z2 = x + y\n",
        "print(z2)\n",
        "\n",
        "z3 = torch.sub(x, y) # for subtraction, or we can also do as\n",
        "print(z3)\n",
        "z4 = x - y\n",
        "print(z4)\n",
        "\n",
        "z5 = torch.div(x, y) # for division, or we can also do as\n",
        "print(z5)\n",
        "z6 = x/y\n",
        "print(z6)\n",
        "\n",
        "z7 = torch.mul(x, y) # for multiplicaion, or we can also do as\n",
        "print(z7)\n",
        "z8 = x * y\n",
        "print(z8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkkycR51crHZ",
        "outputId": "b14e01cd-88b0-46f1-cf5a-07e8fb6b3314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[10, 10],\n",
            "        [10, 10]])\n",
            "tensor([[10, 10],\n",
            "        [10, 10]])\n",
            "tensor([[-6, -4],\n",
            "        [-2,  0]])\n",
            "tensor([[-6, -4],\n",
            "        [-2,  0]])\n",
            "tensor([[0.2500, 0.4286],\n",
            "        [0.6667, 1.0000]])\n",
            "tensor([[0.2500, 0.4286],\n",
            "        [0.6667, 1.0000]])\n",
            "tensor([[16, 21],\n",
            "        [24, 25]])\n",
            "tensor([[16, 21],\n",
            "        [24, 25]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing\n",
        "\n",
        "x = torch.tensor([[2, 3], [4, 5]])\n",
        "print(x[1]) # This will fetch the 1st index tensor from x i.e., we can see [4, 5]\n",
        "print(x[1, 0]) # This wil fetch the 0th index element from 1st index tensor from x i.e., 4 as we can see easily"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmYznHpgetAm",
        "outputId": "a084e7cc-e02d-4bdf-d038-20729821cf6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 5])\n",
            "tensor(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping Tensor:"
      ],
      "metadata": {
        "id": "_TdPmeFIgU2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y = x.view()\n",
        "#It is used to reshape the tensor"
      ],
      "metadata": {
        "id": "ovBicYZrga1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #For ex., x.view(-1, _) --> So the blank space represents no of values we want in single tensor."
      ],
      "metadata": {
        "id": "g5o3EaPngnUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tensor to Numpy**"
      ],
      "metadata": {
        "id": "23d3X5MliZYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print tensor with dimensions 1*5 with ones values\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "print(type(b))\n",
        "\n",
        "# BE CAREFUL because if tensor is always on the CPU and not GPU then all of the objects will share same memory location\n",
        "\n",
        "# For ex:\n",
        "\n",
        "a.add_(1) # then it will add 1 in each tensor a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoJy2doohK6i",
        "outputId": "b1071567-cc65-4800-a6a8-b0ae4e2b21d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "[1. 1. 1. 1. 1.]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2., 2., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Numpy to Tensor**"
      ],
      "metadata": {
        "id": "rNMBty8tjHVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.ones(5)\n",
        "print(a)\n",
        "b = torch.from_numpy(a)\n",
        "print(b)\n",
        "\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKI5js72hhF7",
        "outputId": "2617ff55-8b3d-4fb2-8379-8bde6127008e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Numpy can only handle CPU tensor and we cannot convert GPU tensor back to Numpy"
      ],
      "metadata": {
        "id": "P1QP_a1bjxaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY_tCeBjhm9k",
        "outputId": "086435cd-b965-4983-b070-fc7f8345deb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**requires_grad** -> Tells PyTorch that it will need to calculate gradients for this tensor late in our optimization steps.  \n",
        "So whenever we have a variable in our model that we want to optimize then we need the gradients so we need to specify **requires_grad = True**"
      ],
      "metadata": {
        "id": "v01z5o8NkWGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autograd package in PyTorch**"
      ],
      "metadata": {
        "id": "vJHeWePalAlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x)\n",
        "\n",
        "\n",
        "# We want to calculate gradient of some functionwith respect to x nd we pass requires_grad = True\n",
        "\n",
        "y = x + 2\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHclfHTIhuPT",
        "outputId": "b9313d14-5977-4ae2-866f-97a093b470e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "   X\n",
        "    \\\n",
        "     \\\n",
        "       '+' --> Y\n",
        "     /\n",
        "    /\n",
        "   2\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEC5NKwhl1hM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each operation we have a node of inputs and outputs so here operation is **'+'** and inputs are **x** & **2** and out is **Y**  \n",
        "Now with this graph in the technique called Backpropagation we can do calculate the gradients"
      ],
      "metadata": {
        "id": "EJsTqpognDcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "   Forward pass\n",
        "   ------------->\n",
        "\n",
        "   X            --|\n",
        "    \\             |\n",
        "     \\            |\n",
        "       '+' --> Y  | grad_fn\n",
        "     /            |\n",
        "    /             |\n",
        "   2            --|\n",
        "\n",
        "   <------------\n",
        "   Add Backward\n",
        "```"
      ],
      "metadata": {
        "id": "GPsBlQoRoDJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In forward pass we calculate an output  'Y' and since we specified requiring gradient PyTorch will automatically create and store a functn for us and this function is then used in back propagation and to get the gradients.\n",
        "So here, Y has an attribute grad_fn so this will point to the gradient function 'Add Backward' in our case we named and this function can then calculate the gradients in the backward pass so it will calculate Y with respect to X in this case.\n",
        "When we want to calculate gradients,the only thing we must to call is **z.backward()** so it will calculate Y, dZ/dX and print (x.grad)"
      ],
      "metadata": {
        "id": "7woPhdTLovWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# tensor X with requires_grad=True\n",
        "X = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
        "print(X)\n",
        "\n",
        "# perform computation with X\n",
        "Y = X + 2\n",
        "print(Y)\n",
        "\n",
        "Z = Y * Y * 2\n",
        "Z = Z.mean()\n",
        "print(Z)\n",
        "\n",
        "# Backward pass to compute gradients\n",
        "Z.backward()  # computing gradients dZ/dX\n",
        "print(X.grad)  # gradients of X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dNFzcDMr4wU",
        "outputId": "9a010b56-0bed-4ae4-b5ec-c32d1b71c109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], requires_grad=True)\n",
            "tensor([3., 4., 5.], grad_fn=<AddBackward0>)\n",
            "tensor(33.3333, grad_fn=<MeanBackward0>)\n",
            "tensor([4.0000, 5.3333, 6.6667])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacobian matrix with partial derivatives and we multiply it with a gradient vector and we get the final gradients  \n",
        "For using **backward()** our value must be scalar  "
      ],
      "metadata": {
        "id": "mfRGpCe0qcT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
        "print(X)\n",
        "\n",
        "Y = X + 2\n",
        "print(Y)\n",
        "\n",
        "Z = Y * Y * 2\n",
        "# Z = Z.mean()\n",
        "print(Z)\n",
        "\n",
        "V = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
        "Z.backward(V)\n",
        "print(X.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnfqT1lyiGbD",
        "outputId": "4f18aede-f51a-49ea-9acb-b3dff09c87de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], requires_grad=True)\n",
            "tensor([3., 4., 5.], grad_fn=<AddBackward0>)\n",
            "tensor([18., 32., 50.], grad_fn=<MulBackward0>)\n",
            "tensor([ 1.2000, 16.0000,  0.0200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we prevent from tracking the Gradients?  \n",
        "We have 3 options:  \n",
        "(i) requires_grad_(False)  \n",
        "(ii) x.detach()  \n",
        "(iii) with torch.no_grad()  \n",
        "So using these techniques we can stop gradient functions and tracking history of our computational graph.  \n"
      ],
      "metadata": {
        "id": "H9rK2MHEGGxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whenever we call **backward()** function, then the gradient for this tensor will be accumulated to the **'.grad'** attribute.  \n",
        "So the values will be summed up.  \n",
        "So here we must be very careful."
      ],
      "metadata": {
        "id": "gyYwPMjsGypm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1, 2, 3]\n",
        "x = torch.tensor(a, dtype=torch.float32, requires_grad = True) # need to mention dtype\n",
        "print(x)\n",
        "\n",
        "X = x + 2\n",
        "x.requires_grad_(False)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioC3_Y8miIqb",
        "outputId": "5b2c28bc-cd60-4261-ece0-3a633caaa616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], requires_grad=True)\n",
            "tensor([1., 2., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define tensor x with requires_grad=True\n",
        "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# perform operations on x and store the result in a new variable\n",
        "y = x + 2\n",
        "\n",
        "# detach the new tensor to prevent further gradient tracking\n",
        "y = y.detach()\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT0RXnuyIlVs",
        "outputId": "08cdaaaa-6f19-4bca-df5b-2e07f848355d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], requires_grad=True)\n",
            "tensor([3., 4., 5.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# define tensor x with requires_grad=True\n",
        "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "# perform operations on x without tracking gradients\n",
        "with torch.no_grad():\n",
        "    y = x + 2\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBD3d72MJo5i",
        "outputId": "96cf2dde-4ab7-456d-e273-5aaab73229d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], requires_grad=True)\n",
            "tensor([3., 4., 5.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex:\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(1):\n",
        "  model_output = (weights*3).sum()\n",
        "  model_output.backward()\n",
        "  print(weights.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9AktwiDMV77",
        "outputId": "22576582-4856-4c2a-f9e0-2c2c51d6c115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here is the explanation:  \n",
        "So firstly I created a Tensor 'weights' having size 1*4 and values as 1's i.e., [1, 1, 1, 1], then a loop runs for 1 iteration. In Deep Learning an epoch refers to one forward pass and one backward pass.  \n",
        "Then calculating model's output by multiplying each element in tensor 'weights' by 3, [1, 1, 1, 1] * 3 = [3, 3, 3, 3] and summing up all the elements 3 + 3 + 3 + 3 = 12.  \n",
        "Then it performs back propagation to compute gradients of the loss with respect to the weights (It computes the gradients using automatic differentiation provided by PyTorch). Gradients represent the slope of the loss function with respect to each weight, which guides the optimization process during training.   \n",
        "Forward Pass: model_output = SUMMATION(3 * weights[i])  \n",
        "model_output.backward(): PyTorch computes the gradients of the loss with respect to the weights.  \n",
        "Since loss function in this case is model_output, which is a scalar, the gradients represent how much each weight contributes to the loss.  \n",
        "Since the loss is a simple linear function, the gradient for each weight is simply the coefficient (which is 3 in this case).  \n",
        "So after the backward pass, the gradient for each weight become [3, 3, 3, 3]   \n",
        "Then I directly just print gradients.  \n",
        "\n"
      ],
      "metadata": {
        "id": "rTX8vi4hSBiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I go for 2, 3 or so on iterations, I get reults as:"
      ],
      "metadata": {
        "id": "6t5SE5-scM52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(2):\n",
        "  model_output = (weights*3).sum()\n",
        "  model_output.backward()\n",
        "  print(weights.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftrskY-FR-x9",
        "outputId": "d95db604-857f-432a-b75c-a68cb5ad64e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "  model_output.backward()\n",
        "  print(weights.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPLU_aRKcbKu",
        "outputId": "8fd9517a-46b3-4d48-ba51-9b39cbeb5357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([9., 9., 9., 9.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the weights and gradients we can see clearly are incorrect.  \n",
        "So for next iterations and optimization steps steps, I must need to empty the gradients"
      ],
      "metadata": {
        "id": "P4AsKS4GcrBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "  weights.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gqz1N9eccul",
        "outputId": "c14830bb-8684-4ce2-cf08-e2b278380f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch Built-in optimizer**"
      ],
      "metadata": {
        "id": "0Vhl0xvddmFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "print(\"Initial weights:\")\n",
        "print(weights)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
        "\n",
        "# performing a single optimization step\n",
        "optimizer.step()\n",
        "print(\"Weights after optimization step:\")\n",
        "print(weights)\n",
        "\n",
        "# zero out the gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# verify that gradients are zeroed out\n",
        "print(\"Gradients after zero_grad:\")\n",
        "print(weights.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4QQqpJtdVl1",
        "outputId": "3cd2239f-f76b-48d3-dfce-64a5f91ee636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights:\n",
            "tensor([1., 1., 1., 1.], requires_grad=True)\n",
            "Weights after optimization step:\n",
            "tensor([1., 1., 1., 1.], requires_grad=True)\n",
            "Gradients after zero_grad:\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD -> Stochastic Gradient Descent having weights as parameters and learning rate."
      ],
      "metadata": {
        "id": "1WKuspFsfOdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This we do exactly same  \n",
        "Whenever we wants to calculate gradients, we must specify required_grad as True. Then we can simply calculate the gradients with **backward()** function and before we want to perform the next operation or next iteration our optimization steps we must empty our gradients **'.grad.zero()'**"
      ],
      "metadata": {
        "id": "fhoahI5IH50k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Backpropagation:**\n",
        "### Chain rule:  \n",
        "\n",
        "\n",
        "```\n",
        "   input  +----------+   output(y) input(y)  +----------+  output(z)\n",
        "--------->|   a(x)   |----------><---------->|   b(y)   |---------->\n",
        "          +----------+                       +----------+   \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-KMb742bImt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We give input **'x'** and apply thjis in function **a(x)** then gets the output **'y'**, this output gets new input and is applied on the funcion **'b(y)'** and gets new output **'z'**"
      ],
      "metadata": {
        "id": "JneBXiD8K3Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to minimize 'z'. So we want to know the revertive of 'z' with respect to 'x' and we can do this using chain rule.  \n",
        "**dz/dx = (dz/dy) * (dy/dx)**"
      ],
      "metadata": {
        "id": "xHQ7xDSSLk25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computational graph:  \n",
        "\n",
        "\n",
        "```\n",
        "+-----+\n",
        "|     |\n",
        "+-----+\n",
        "       \\\n",
        "        \\ x\n",
        "         \\         +-----+     z\n",
        "          ---------|  f  |---------->\n",
        "         /         +-----+\n",
        "        / y\n",
        "       /\n",
        "+-----+\n",
        "|     |\n",
        "+-----+\n",
        "```\n",
        "For every operation we do with tensor, pytorch creates a graph for us. So at each node we apply one operation or function with some inputs & then get an output.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[dz/dx] = (dx.y/dx) = y\n",
        "X\n",
        " \\\n",
        "  \\\n",
        "   \\\n",
        "    +-----+\n",
        "    |  f  |     z\n",
        "    |     |---------->\n",
        "    | x*y |\n",
        "    +-----+\n",
        "   /\n",
        "  /\n",
        " /\n",
        "y\n",
        "[dz/dy] = (dy.x/dy) = x\n",
        "\n",
        "[dz/dx] = (dx.y/dx) = y\n",
        "                       \\__________ logical gradients\n",
        "                       /\n",
        "[dz/dy] = (dy.x/dy) = x\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V_I-Z9R8MS-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Why to Graph?**  \n",
        "Our graph has more operations and at the end we calculate the loss function that we want to minimize.  \n",
        "So we have to calculate the gradients of this loss with respect to our parameters 'x' inmm the beginning."
      ],
      "metadata": {
        "id": "uws8OrbDkDZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whole concept consists of 3 steps:  \n",
        "i. Forward pass: Compute loss.  \n",
        "ii. Compute local gradients  \n",
        "iii. Backward Pass: Compute [**d(loss)/d(weights)**] using chain rule."
      ],
      "metadata": {
        "id": "BKNEwYnZpTMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ex: Linear Regreassion (MSE)  \n",
        "y_prediction = w . x  \n",
        "loss = (y_prediction - y)^2 = (wx - y)  "
      ],
      "metadata": {
        "id": "_-SVrl6VsM4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "x\n",
        " \\\n",
        "  \\\n",
        "   \\            +-----+   y_pred  +-----+     S     +-----+   loss\n",
        "    |---------->|  *  |---------->|  -  |---------->|  ^2 |---------->\n",
        "   /            +-----+           +-----+           +-----+\n",
        "  /                                  ^\n",
        " /                              y_____|\n",
        "w\n",
        "```\n",
        "\n",
        "Forward pass x=1, y=2, w=1\n",
        "\n",
        "y_pred = 1 * 1 = 1  \n",
        "S = y_pred - y = 1 -2 = -1  \n",
        "(S)^2 = (-1)^2 = 1  \n",
        "  \n",
        "d(y_pred) / dw = (dw . x) / dw = x  \n",
        "d(S) / d(y_pred) = d(y_pred - y) / d(y_pred) = 1  \n",
        "d(loss) / d(S) = d(S)^2 / dS = 2S  \n",
        "  \n",
        "Backward Pass:  \n",
        "d(loss) / d(y_pred) = [d(loss) / d(S)].[d(S) / d(y_pred)] = 2S * 1 = -2 (as S = -1)  \n",
        "d(loss) / dw = [d(loss) / dy].[d(y_pred) / dw] = -2.x = -2 * 1 = -2 (as x = 1)"
      ],
      "metadata": {
        "id": "SR889feQzI9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement with an example:"
      ],
      "metadata": {
        "id": "h6BrwJwS6dH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#Data\n",
        "X = torch.tensor(1.0)\n",
        "Y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) #weights\n",
        "\n",
        "#Forward Pass and compute the loss\n",
        "y_pred = w * X\n",
        "loss = (y_pred - Y)**2\n",
        "print(loss)\n",
        "\n",
        "#Backward Pass\n",
        "loss.backward()\n",
        "print(w.grad)"
      ],
      "metadata": {
        "id": "k57jJfnReKcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619b5cd7-e09e-4136-95bb-8cc067a4f24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Descent with Autograd and Backpropagation**  \n",
        "  \n",
        "------------------------------------------------\n",
        "  \n",
        "Prediction: Manually  \n",
        "Gradients Computation: Manually  \n",
        "Loss Computation: Manually  \n",
        "Parameters update: Manually    \n",
        "\n",
        "------------------------------------------------  \n",
        "\n",
        "Prediction: Manually  \n",
        "Gradients Computation: Autograd  \n",
        "Loss Computation: Manually  \n",
        "Parameters update: Manually   \n",
        "  \n",
        "------------------------------------------------  \n",
        "  \n",
        "Prediction: Manually  \n",
        "Gradients Computation: Autograd  \n",
        "Loss Computation: PyTorch Loss  \n",
        "Parameters update: PyTorch Optimizer  \n",
        "\n",
        "------------------------------------------------  \n",
        "\n",
        "Prediction: PyTorch Model  \n",
        "Gradients Computation: Autograd  \n",
        "Loss Computation: PyTorch Loss  \n",
        "Parameters update: PyTorch Optimizer  "
      ],
      "metadata": {
        "id": "kh30l9MP6lG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction: Manually\n",
        "# Gradients Computation: Manually\n",
        "# Loss Computation: Manually\n",
        "# Parameters update: Manually\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#f = w*x\n",
        "#f = 2*x\n",
        "x = np.array([1, 2, 3, 4], dtype = np.float32)\n",
        "y = np.array([2, 4, 6, 8], dtype = np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "#model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "#Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "#Gradient\n",
        "#MSE = 1/N (w*x - y)**2\n",
        "#dj/dw = 1/N (2*x)(w*x - y)\n",
        "def gradient (x, y, y_predicted):\n",
        "  return np.dot(2*x, y_predicted - y).mean()\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = forward pass\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  #loss\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  #gradients\n",
        "  dw = gradient(x, y, y_pred)\n",
        "\n",
        "  #update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch{epoch + 1}:w = {w:.3f}, loss={l:.8f}')\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJBW7YVj86ZP",
        "outputId": "cbad91d6-dcdb-4a2f-fc9d-2989af1cc458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch1:w = 1.200, loss=30.00000000\n",
            "epoch2:w = 1.680, loss=4.79999924\n",
            "epoch3:w = 1.872, loss=0.76800019\n",
            "epoch4:w = 1.949, loss=0.12288000\n",
            "epoch5:w = 1.980, loss=0.01966083\n",
            "epoch6:w = 1.992, loss=0.00314574\n",
            "epoch7:w = 1.997, loss=0.00050331\n",
            "epoch8:w = 1.999, loss=0.00008053\n",
            "epoch9:w = 1.999, loss=0.00001288\n",
            "epoch10:w = 2.000, loss=0.00000206\n",
            "epoch11:w = 2.000, loss=0.00000033\n",
            "epoch12:w = 2.000, loss=0.00000005\n",
            "epoch13:w = 2.000, loss=0.00000001\n",
            "epoch14:w = 2.000, loss=0.00000000\n",
            "epoch15:w = 2.000, loss=0.00000000\n",
            "epoch16:w = 2.000, loss=0.00000000\n",
            "epoch17:w = 2.000, loss=0.00000000\n",
            "epoch18:w = 2.000, loss=0.00000000\n",
            "epoch19:w = 2.000, loss=0.00000000\n",
            "epoch20:w = 2.000, loss=0.00000000\n",
            "Prediction before training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:  \n",
        "**Step 1: Import necessary libraries:**  \n",
        "So firstly, we  import the necessary libraries,   \n",
        "**numpy** -> numerical computations and   \n",
        "**torch** -> using PyTorch (though in this code, PyTorch isn't actually used).    \n",
        "  \n",
        "**Step 2: Define data and initial parameters:**   \n",
        "**x** and **y** are arrays of input and output data.  \n",
        "**x** -> input  \n",
        "**y** -> expected output  \n",
        "**w** -> weight of our model, initially set to 0.  \n",
        "  \n",
        "**Step 3: Define model prediction function:**  \n",
        "function takes 'x' as input and returns predicted output by multiplying 'x' with weight 'w'  \n",
        "  \n",
        "**Step 4: Define loss function:**  \n",
        "loss function calculates how far off our predictions are from the actual values using mean squared error (MSE).  \n",
        "  \n",
        "**Step 5: Define Gradient Calculation:**  \n",
        "This function calculates the gradient, which tells how to change 'w' to reduce the loss.  \n",
        "Formula used is derived from the derivative of the loss function with respect to 'w'.   \n",
        "  \n",
        "**Step 6: Print Initial Prediction:**  \n",
        "Before training, we check model's prediction for x=5. With 'w' being 0, this will output 0.  \n",
        "  \n",
        "**Step 7: Training Loop:**  \n",
        "**learning_rate** -> how big of a step we take when updating 'w'  \n",
        "**n_items** -> number of times we go through the training loop.  \n",
        "  \n",
        "**Step 8: Training process:**  \n",
        "For each iteration (epoch) in the training loop  \n",
        "**Prediction** -> Calculate the predicted 'y' values using the current weight 'w'.    \n",
        "**Loss** -> Calculate the loss using predicted 'y' and actual 'y'.  \n",
        "**Gradient** -> Calculate the gradient to understand how to adjust 'w'.  \n",
        "**Update weights** -> Adjust 'w' by subtracting the product of 'learning_rate' and 'dw'.   \n",
        "**Print Progress** -> Every epoch, print the current epoch number, weight and loss.  \n",
        "  \n",
        "**Step 9: Print final Prediction:**  \n",
        "After training, we check the model's prediction for x = 5 again see hoow much it improved."
      ],
      "metadata": {
        "id": "xyiZhLyyGqDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we start with an initial weight 'w' of 0.  \n",
        "We predict values, calculate loss, find the gradient and update 'w' iteratively.  \n",
        "After several iterations, our model's weight 'w' should improve, reducing the loss and making better predictions."
      ],
      "metadata": {
        "id": "CZWpc95pYHV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction: Manually\n",
        "# Gradients Computation: Autograd\n",
        "# Loss Computation: Manually\n",
        "# Parameters update: Manually\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#f = w*x\n",
        "#f = 2*x\n",
        "x = np.array([1, 2, 3, 4], dtype = np.float32)\n",
        "y = np.array([2, 4, 6, 8], dtype = np.float32)\n",
        "\n",
        "w = 0.0\n",
        "\n",
        "#model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "#Loss = MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "#Gradient\n",
        "#MSE = 1/N (w*x - y)**2\n",
        "#dj/dw = 1/N (2*x)(w*x - y)\n",
        "def gradient (x, y, y_predicted):\n",
        "  return np.dot(2*x, y_predicted - y).mean()\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = forward pass\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  #loss\n",
        "  l = loss(y, y_pred)\n",
        "\n",
        "  #gradients\n",
        "  dw = gradient(x, y, y_pred)\n",
        "\n",
        "  #update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch{epoch + 1}:w = {w:.3f}, loss={l:.8f}')\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "id": "RJxdUMix_euT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ccdc8f0-752c-4878-a832-919a938f89cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch1:w = 1.200, loss=30.00000000\n",
            "epoch3:w = 1.872, loss=0.76800019\n",
            "epoch5:w = 1.980, loss=0.01966083\n",
            "epoch7:w = 1.997, loss=0.00050331\n",
            "epoch9:w = 1.999, loss=0.00001288\n",
            "epoch11:w = 2.000, loss=0.00000033\n",
            "epoch13:w = 2.000, loss=0.00000001\n",
            "epoch15:w = 2.000, loss=0.00000000\n",
            "epoch17:w = 2.000, loss=0.00000000\n",
            "epoch19:w = 2.000, loss=0.00000000\n",
            "Prediction before training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we make changes in:\n",
        "```\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch{epoch + 1}:w = {w:.3f}, loss={l:.8f}')\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
        "```\n",
        "So it will run epochs (iterations) only for odd numbers.  \n",
        "Now this is the implementation where we did everything manually and now let's replace the gradient calculation.  \n",
        "We don't use numpy anymore, so will just use torch and use PyTorch."
      ],
      "metadata": {
        "id": "PDGwhraBY92D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction: Manually\n",
        "# Gradients Computation: Autograd\n",
        "# Loss Computation: PyTorch Loss\n",
        "# Parameters update: PyTorch Optimizer\n",
        "\n",
        "\n",
        "import torch\n",
        "#f = w * x\n",
        "#f = 2 * x\n",
        "\n",
        "x = torch.tensor([1, 2, 3, 4], dtype = torch.float32)\n",
        "y = torch.tensor([2, 4, 6, 8], dtype = torch.float32) #\n",
        "\n",
        "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "#Model Prediction\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#Loss = MSE\n",
        "def Loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training : (5) = {forward(5):.3f}')\n",
        "\n",
        "#Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #Prediction = forward pass\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  #Loss\n",
        "  l = Loss(y, y_pred)\n",
        "\n",
        "  #Gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  #Update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  #Zero gradients\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch{epoch + 1}:w = {w:.3f}, loss = {l:.8f}')\n",
        "print(f'Prediction after training : f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q8-7r3pBKzZ",
        "outputId": "7f7baad0-bbde-421b-a9b2-40b2204ccd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training : (5) = 0.000\n",
            "epoch1:w = 0.300, loss = 30.00000000\n",
            "epoch3:w = 0.772, loss = 15.66018772\n",
            "epoch5:w = 1.113, loss = 8.17471695\n",
            "epoch7:w = 1.359, loss = 4.26725292\n",
            "epoch9:w = 1.537, loss = 2.22753215\n",
            "epoch11:w = 1.665, loss = 1.16278565\n",
            "epoch13:w = 1.758, loss = 0.60698116\n",
            "epoch15:w = 1.825, loss = 0.31684780\n",
            "epoch17:w = 1.874, loss = 0.16539653\n",
            "epoch19:w = 1.909, loss = 0.08633806\n",
            "Prediction after training : f(5) = 9.612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RE7XsICtcoTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So,  \n",
        "1st step:  \n",
        "To design our model. So we design the number of inputs and outputs (input size, output size, forward pass). Also we design forward pass for all the different layers.  \n",
        "  \n",
        "2nd step:  \n",
        "We construct loss and optimizer  \n",
        "  \n",
        "3rd step:  \n",
        "Training loop by doing:  \n",
        "- forward pass: compute prediction.  \n",
        "- backward pass: get the gradients.  \n",
        "- Then update the weights."
      ],
      "metadata": {
        "id": "03PJcLJ7wbid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now, let's replace loss and the optimization.  \n",
        "For this we import neural network module.\n",
        "```\n",
        "import torch.nn as nn\n",
        "```\n",
        "Now we don't need to decide loss manually anymore. So we can simply remove this.  \n",
        "Before our training, we still need to decide our loss  \n",
        "loss = nn.MSELoss()  \n",
        "Then we also need an optimizer from PyTorch    \n",
        "optimizer = torch.optim.SGD([w], lr = learning_rate)    \n",
        "SGD -> Stochastic Gradient Descent  \n",
        "which needs some parameters that it should optimize and it will nede this as a list.  \n",
        "So, we put our **w** and also **lr** which is learning rate.  \n",
        "Now, going towards our training loop  \n",
        "loss computation is still not the same, because this is a callable function which gets the actual y and predicted_y and then we don't need to manually update our weights anymore.  \n",
        "So we can simply do:  \n",
        "optimizer.step() which will do optimization steps.  \n",
        "  \n",
        "And, also we still have to empty our gradients after the optimization steps  \n",
        "We can say  \n",
        "optimizer.zero_grad()  \n",
        "And now we are done with step 3.  \n",
        "  \n",
        "Let's continue with the stepp 4 and replace our manually implemented forward method a PyTorch method.  \n",
        "  \n",
        "So for this we also don't need weights anymore because our PyTorch model knows the parameters.  \n",
        "So, here we say:  \n",
        "model = nn.Linear()  \n",
        "This needs an input size and output size of our features and for this we need to do some modifications.  \n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SqWQO6-BxmiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction: PyTorch Model\n",
        "# Gradients Computation: Autograd\n",
        "# Loss Computation: PyTorch Loss\n",
        "# Parameters update: PyTorch Optimizer"
      ],
      "metadata": {
        "id": "YN3mfBWqqe3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Design model (input, output size, forward pass)\n",
        "# 2) Construct loss and optimizer\n",
        "# 3) Training loop\n",
        "# - forward pass -> compute prediction\n",
        "# - backward pass -> gradients\n",
        "# - update weights"
      ],
      "metadata": {
        "id": "wVSIdXhqIc9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So now, let's replace loss and the optimization.\n",
        "# For this we import neural network module.\n",
        "# Step 1:\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#f = w * x\n",
        "#f = 2 * x\n",
        "\n",
        "\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "# Step 2:\n",
        "# So our x and y needs to have a different shapes. So this must be a 2D array now. With a number of rows is the number of samples and for each row we have the number of features.\n",
        "\n",
        "x = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)\n",
        "y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
        "x = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)\n",
        "y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)\n",
        "\n",
        "x_test = torch.tensor([5], dtype = torch.float32)\n",
        "\n",
        "# Let's get the shape:\n",
        "# num_samples, num_features = x.shape\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "\n",
        "# Now, we need to give this to our model. So we say our input size and output size then we want to get the prediction we can simply say we can call them model.\n",
        "# But now this cannot have a float value so this must be a tensor.\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "\n",
        "# Now, we define our model. So this needs an input and output size is still the same. So this is also number of features.\n",
        "\n",
        "model = nn.Linear(input_size, output_size) #After Step 4\n",
        "\n",
        "# So let's create a test tensor let's say:\n",
        "# X_test = torch.tensor which gets only one sample with 5 and then its get a datatype of say torch.float32 and then we pass this X_test sample in model.\n",
        "# Since this is the only one val has only one value we can call it as .item():.3f to get actual float value name.\n",
        "# Same we do for backward pass.\n",
        "\n",
        "# WE SET X_test IN STEP 1 and USE IT IN THIS STEP ITSELF i.e in STEP 2 IN LINE:\n",
        "# print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# And now we also have to modify our optimizer here. So we don't have a weights now. So this lists with the parameter. We can simply say:\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "# Now, for prediiction we also simply call model and now we are done, so now we are going to use pytorch model. So now we are using PyTorch model.\n",
        "\n",
        "# for epoch in range(n_iters):\n",
        "#   #Prediction = forward pass\n",
        "#   y_prediction = model(x)\n",
        "\n",
        "# THIS ALL MUST BE DONE IN STEP 4\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "#Step 3:\n",
        "#Training\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "#Now we don't need to decide loss manually anymore. So we can simply remove this.\n",
        "# Before our training, we still need to decide our loss\n",
        "# loss = nn.MSELoss()\n",
        "\n",
        "# Then we also need an optimizer from PyTorch\n",
        "# optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
        "# SGD -> Stochastic Gradient Descent\n",
        "# which needs some parameters that it should optimize and it will nede this as a list.\n",
        "# So, we put our w and also lr which is learning rate.\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "#Step 4:\n",
        "# Now, going towards our training loop\n",
        "# loss computation is still not the same, because this is a callable function which gets the actual y and predicted_y and then we don't need to manually update our weights anymore.\n",
        "for epoch in range(n_iters):\n",
        "  #Prediction = forward pass\n",
        "  y_prediction = model(x)\n",
        "\n",
        "  #Loss\n",
        "  l = loss(y, y_prediction)\n",
        "\n",
        "  #Gradients = backward pass\n",
        "  l.backward() #dl/dw\n",
        "\n",
        "  # So we can simply do:\n",
        "  # optimizer.step() which will do optimization steps.\n",
        "\n",
        "  #Update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # And, also we still have to empty our gradients after the optimization steps\n",
        "  # We can say\n",
        "  # optimizer.zero_grad()\n",
        "\n",
        "  #Zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # And now we are done with step 4.\n",
        "  # Let's continue with the stepp 4 and replace our manually implemented forward method a PyTorch method.\n",
        "  # So for this we also don't need weights anymore because our PyTorch model knows the parameters.\n",
        "  # So, here we say:\n",
        "  # model = nn.Linear() in step 2\n",
        "  # This needs an input size and output size of our features and for this we need to do some modifications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "#Step 5:\n",
        "\n",
        "# To get this and also if we want to print them we have to print them again.\n",
        "# if epoch % 10 == 0:\n",
        "#   [w, b] = model.parameters()\n",
        "# This will unpack them and then if we want to print the actual, this will be a list of lists.\n",
        "# So lets get the first or the actual first weight and we can also call the item() because we don't want to see tensor.\n",
        "# print(f'epoch{epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch{epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "\n",
        "# This is how we construct training pipeline.\n",
        "# In this case we didn't have to come up with the model for ourselves. So here we we only have one layer and this is already provided in PyTorch.\n",
        "\n",
        "# model = nn.Linear (input_size, output_size)"
      ],
      "metadata": {
        "id": "ksxXvG9SJSYU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "317c703a-f79c-409d-c66c-d4c4d7f8b968"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = -4.217\n",
            "epoch1: w = -0.430, loss = 60.30207443\n",
            "epoch11: w = 1.356, loss = 1.69045520\n",
            "epoch21: w = 1.650, loss = 0.16644992\n",
            "epoch31: w = 1.705, loss = 0.11987763\n",
            "epoch41: w = 1.721, loss = 0.11194585\n",
            "epoch51: w = 1.730, loss = 0.10540530\n",
            "epoch61: w = 1.739, loss = 0.09926949\n",
            "epoch71: w = 1.746, loss = 0.09349152\n",
            "epoch81: w = 1.754, loss = 0.08804980\n",
            "epoch91: w = 1.761, loss = 0.08292492\n",
            "Prediction after training: f(5) = 9.521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:  \n",
        "So our x and y needs to have a different shapes. So this must be a 2D array now. With a number of rows is the number of samples and for each row we have the number of features.  \n",
        "x = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32)   \n",
        "y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32)  \n",
        "  \n",
        "Let's get the shape:  \n",
        "num_samples, num_features = x.shape  \n",
        "  \n",
        "Now, we define our model. So this needs an input and output size is still the same. So this is also number of features.  \n",
        "Now, we need to give this to our model. So we say our input size and output size then we want to get the prediction we can simply say we can call them  model. But now this cannot have a float value so this must be a tensor.  \n",
        "  \n",
        "So let's create a test tensor let's say:  \n",
        "X_test = torch.tensor which gets only one sample with 5 and then its get a datatype of say torch.float32 and then we pass this X_test sample in model. Since this is the only one val has only one value we can call it as .item():.3f to get actual float value name.  \n",
        "Same we do for backward pass.  \n",
        "And now we also have to modify our optimizer here. So we don't have a weights now. So this lists with the parameter. We can simply say:  \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)  \n",
        "Now, for prediiction we also simply call model and now we are done, so now we are going to use pytorch model. So now we are using PyTorch model.  \n",
        "```\n",
        "for epoch in range(n_iters):\n",
        "  #Prediction = forward pass\n",
        "  y_prediction = model(x)\n",
        "```\n",
        "To get this and also if we want to print them we have to print them again.  \n",
        "```\n",
        "if epoch % 10 == 0:\n",
        "  [w, b] = model.parameters()\n",
        "```\n",
        "This will unpack them and then if we want to print the actual, this will be a list of lists.  \n",
        "So lets get the first or the actual first weight and we can also call the item() because we don't want to see tensor.  \n",
        "\n",
        "\n",
        "```\n",
        "print(f'epoch{epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "```\n",
        "This is how we construct training pipeline.  \n",
        "In this case we didn't have to come up with the model for ourselves. So here we we only have one layer and this is already provided in PyTorch.  \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model = nn.Linear (input_size, output_size)\n",
        "```\n",
        "\n",
        "But let's say we need custom model. Then we can do like:"
      ],
      "metadata": {
        "id": "VTA5keNDNo9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries and define the model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "# Step 2: Define the data and initialize the model\n",
        "\n",
        "# Define input and output tensors\n",
        "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "# Define a test input tensor\n",
        "x_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "# Get the number of samples and features from the input tensor\n",
        "n_samples, n_features = x.shape\n",
        "print(n_samples, n_features)  # Output: 4 1\n",
        "\n",
        "# Define the Linear Regression model\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        # Define the linear layer with input and output dimensions\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass\n",
        "        return self.lin(x)\n",
        "\n",
        "# Initialize the model\n",
        "input_size = n_features\n",
        "output_size = n_features  # This should be 1 because we are predicting a single value\n",
        "model = LinearRegression(input_size, 1)\n",
        "\n",
        "# Print the initial prediction before training\n",
        "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "# Step 3: Define the loss function and the optimizer\n",
        "\n",
        "# Set the learning rate and the number of iterations\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "# Define the Mean Squared Error (MSE) loss function\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# Define the Stochastic Gradient Descent (SGD) optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "# Step 4: Training loop\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # Prediction = forward pass\n",
        "    y_prediction = model(x)\n",
        "\n",
        "    # Compute the loss\n",
        "    l = loss(y, y_prediction)\n",
        "\n",
        "    # Compute gradients = backward pass\n",
        "    l.backward()  # Compute the gradients of the loss with respect to the model parameters\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Zero the gradients to prevent accumulation\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Print loss and weights every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        [w, b] = model.parameters()\n",
        "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "#=================================================================================================================================================================================\n",
        "# Step 5: Print the prediction after training\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "# Explanation:\n",
        "# 1. We imported the necessary libraries.\n",
        "# 2. We defined the input (x) and output (y) tensors and a test input (x_test).\n",
        "# 3. We created a simple Linear Regression model with one linear layer.\n",
        "# 4. We defined the MSE loss function and the SGD optimizer with a specified learning rate.\n",
        "# 5. We trained the model over a number of iterations (n_iters).\n",
        "#    - In each iteration, we performed a forward pass to get the predictions.\n",
        "#    - We computed the loss between the predictions and the actual values.\n",
        "#    - We performed a backward pass to compute the gradients.\n",
        "#    - We updated the model's parameters using the optimizer.\n",
        "#    - We zeroed the gradients to prevent them from accumulating.\n",
        "#    - We printed the weights and loss every 10 epochs for monitoring.\n",
        "# 6. We printed the prediction for x_test (5) before and after training to see the improvement.\n",
        "\n",
        "\n",
        "# Explanation in a mathematical manner:\n",
        "# Let's denote:\n",
        "# - `x` as the input features.\n",
        "# - `y` as the target values.\n",
        "# - `w` as the weights of the model.\n",
        "# - `b` as the bias of the model.\n",
        "# - `learning_rate` as the learning rate (η).\n",
        "# - `n_iters` as the number of iterations.\n",
        "# - `l` as the loss function (Mean Squared Error).\n",
        "\n",
        "# In the context of linear regression, the model prediction `y_pred` is given by:\n",
        "# y_pred = w * x + b\n",
        "\n",
        "# The Mean Squared Error (MSE) loss function is:\n",
        "# MSE = (1/N) * Σ(y - y_pred)^2\n",
        "# where N is the number of samples.\n",
        "\n",
        "# During each iteration of the training loop, the following steps are performed:\n",
        "\n",
        "# 1. Forward pass: Compute the predicted values using the current weights and bias.\n",
        "#    y_pred = w * x + b\n",
        "# 2. Compute the loss: Calculate the difference between the predicted values and the actual target values.\n",
        "#    l = (1/N) * Σ(y - y_pred)^2\n",
        "# 3. Backward pass: Compute the gradients of the loss with respect to the model parameters (w and b).\n",
        "#    dl/dw = (2/N) * Σ((y_pred - y) * x)\n",
        "#    dl/db = (2/N) * Σ(y_pred - y)\n",
        "# 4. Update weights: Adjust the weights and bias using the computed gradients and the learning rate.\n",
        "#    w = w - η * dl/dw\n",
        "#    b = b - η * dl/db\n",
        "# 5. Zero the gradients: Reset the gradients to zero to prevent accumulation from previous iterations.\n",
        "# These steps are repeated for a specified number of iterations (n_iters) until the model converges to the optimal weights and bias that minimize the loss function.\n",
        "# After training, the model parameters (w and b) should be optimized, resulting in a better prediction for the test input."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7_SmbhRQMvwB",
        "outputId": "bc5877ea-618f-4c4e-ff58-e330a79a4fc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training: f(5) = -4.574\n",
            "epoch 1: w = -0.535, loss = 62.21440506\n",
            "epoch 11: w = 1.279, loss = 1.80817914\n",
            "epoch 21: w = 1.581, loss = 0.23376960\n",
            "epoch 31: w = 1.638, loss = 0.18215217\n",
            "epoch 41: w = 1.656, loss = 0.17056650\n",
            "epoch 51: w = 1.667, loss = 0.16061319\n",
            "epoch 61: w = 1.677, loss = 0.15126398\n",
            "epoch 71: w = 1.687, loss = 0.14245968\n",
            "epoch 81: w = 1.696, loss = 0.13416776\n",
            "epoch 91: w = 1.705, loss = 0.12635852\n",
            "Prediction after training: f(5) = 9.409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        #define layers\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "    def forward(self, X):\n",
        "        return self.lin(X)\n",
        "```\n",
        "Let's write a custom linear regression model. Then we have to derive this from 'nn.Module' & this will get an ____init__() method which has self and which gets the input_dim -> input dimension and output_dim -> output dimension.  \n",
        "____init__(self, input_dim, output_dim)   \n",
        "Then we call super -> superclass of linear regression with self and then .____init__() this is how we call super constructor and then we define our layers.  \n",
        "`self.lin = nn.Linear(input_dim, output_dim)`\n",
        "And then we also have to implement the forward pass in our model class. So,  \n",
        "\n",
        "\n",
        "```\n",
        "def forward(self, X):\n",
        "    return self.lin(X)\n",
        "```\n",
        "Now, we can do:  \n",
        "\n",
        "\n",
        "```\n",
        "model = LinearRegression(input_size, output_size)\n",
        "```\n",
        "This is how we decide our PyTorch model."
      ],
      "metadata": {
        "id": "4-vB8ZxGNnLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LINEAR REGRESSION**"
      ],
      "metadata": {
        "id": "o_iRBnRdY53_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Step 1:\n",
        "# Design the model\n",
        "# Prepare a dataset\n",
        "x_numpy, y_numpy = datasets.make_regression(n_samples = 100, n_features = 1, noise = 20, random_state = 1)\n",
        "\n",
        "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "print(\"Shape of x is: \", x.shape)\n",
        "print(\"Shape of y is: \", y.shape)\n",
        "print(\"Shape of n_samples is: \", n_samples)\n",
        "print(\"Shape of n_features is: \", n_features)\n",
        "\n",
        "\n",
        "# 1) model:\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Forward pass\n",
        "  y_predicted = model(x)\n",
        "\n",
        "  loss = criterion(y_predicted, y)\n",
        "\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Update\n",
        "  optimizer.step()\n",
        "\n",
        "  # Zero Gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    print(f'\\n epoch: {epoch + 1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# Plot\n",
        "predicted = model(x).detach().numpy()\n",
        "plt.plot(x_numpy, y_numpy, 'ro')\n",
        "plt.plot(x_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "id": "6v7lrqm7Y_22",
        "outputId": "952b00a0-79a9-411f-fc82-6cece8ca9c5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x is:  torch.Size([100, 1])\n",
            "Shape of y is:  torch.Size([100, 1])\n",
            "Shape of n_samples is:  100\n",
            "Shape of n_features is:  1\n",
            "\n",
            " epoch: 10, loss = 4451.3452\n",
            "\n",
            " epoch: 20, loss = 3319.6038\n",
            "\n",
            " epoch: 30, loss = 2500.7759\n",
            "\n",
            " epoch: 40, loss = 1907.7103\n",
            "\n",
            " epoch: 50, loss = 1477.7345\n",
            "\n",
            " epoch: 60, loss = 1165.7139\n",
            "\n",
            " epoch: 70, loss = 939.0977\n",
            "\n",
            " epoch: 80, loss = 774.3812\n",
            "\n",
            " epoch: 90, loss = 654.5707\n",
            "\n",
            " epoch: 100, loss = 567.3660\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Steps performed:**  \n",
        "**Prepare Dataset:**  \n",
        "datasets.make_regression generates a random linear dataset with specified noise.\n",
        "Convert the generated NumPy arrays to PyTorch tensors and ensure y is reshaped into a column vector.\n",
        "  \n",
        "**Define Model:**  \n",
        "A simple linear regression model is defined using nn.Linear, which takes the input size (number of features) and output size.    \n",
        "  \n",
        "**Define Loss and Optimizer:**    \n",
        "Mean Squared Error (MSE) is used as the loss function, appropriate for regression tasks.\n",
        "Stochastic Gradient Descent (SGD) is chosen as the optimizer, with a specified learning rate.\n",
        "\n",
        "**Training Loop:**  \n",
        "For each epoch:  \n",
        "Perform a forward pass to compute predictions.\n",
        "Calculate the loss.  \n",
        "Perform a backward pass to compute gradients.  \n",
        "Update the model weights using the optimizer.  \n",
        "Zero the gradients to prevent accumulation.  \n",
        "Print the loss every 10 epochs for monitoring.  \n",
        "  \n",
        "**Plot Results:**  \n",
        "After training, the predictions are detached from the computation graph and converted back to NumPy for plotting.  \n",
        "The original data points are plotted in red, and the model's predictions are plotted in blue.  "
      ],
      "metadata": {
        "id": "3qkR4m02KuSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**  \n",
        "Typical Pipeline consistes of three steps:  \n",
        "(i) Design model (input, output, forward pass)  \n",
        "(ii) Construct loss and optimizer  \n",
        "(iii) Training loop:  \n",
        "- forward pass: compute prediction and loss\n",
        "- backward pass: gradients  \n",
        "- update weights  \n",
        "  \n",
        "```\n",
        "x_numpy, y_numpy = datasets.make_regression(n_samples = 100, n_features = 1, noise = 20, random_state = 1)\n",
        "\n",
        "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "```\n",
        "Then we should convert these to torch tensor so we say,  \n",
        "\n",
        "Now we convert this to float 32 data type but currently its double datatype so if we use double here, it may occur some errors because it is numpy array (datasets made in numpy arrays). Same for y.  \n",
        "```\n",
        "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "```\n",
        "  \n",
        "Then we reshape our y because right now it is an only one row and we want to make it a column vector. So we want to put each value in row and whole shape has only one column.\n",
        "\n",
        "```\n",
        "y = y.view(y.shape[0], 1)\n",
        "```\n",
        "\n",
        "Shape of x (for ex: (3, 4)) i.e., 3 rows and 4 columns.  \n",
        "So we set same value i.e., shape of x as n samples, n_features like   \n",
        "n_samples = 3  \n",
        "n_features = 4  \n",
        "  \n",
        "So in our dataset there are overall 2 columns for x and y  \n",
        "1 column for x and 1 for y  \n",
        "And each have 100 rows or we can say values  \n",
        "So, n_samples = 3 and n_features = 1\n",
        "```\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "print(\"Shape of x is: \", x.shape)\n",
        "print(\"Shape of y is: \", y.shape)\n",
        "print(\"Shape of n_samples is: \", n_samples)\n",
        "print(\"Shape of n_features is: \", n_features)\n",
        "```\n",
        "\n",
        "Let's create a model now.  \n",
        "So we define a model  \n",
        "In Linear Regression case its just one layer so we can use built-in Linear model   \n",
        "\n",
        "\n",
        "```\n",
        "model = nn.Linear()\n",
        "```\n",
        "\n",
        "This is a linear layer and it needs input size and output size  \n",
        "input_size -> no. of samples (no. of columns in x)  \n",
        "output_size -> np. of features (no. of columns in y)  \n",
        "\n",
        "```\n",
        "model = nn.Linear(input_size, output_size)\n",
        "```\n",
        "Then we will be working on loss and optimizer  \n",
        "  \n",
        "We will be using a built-in loss function from PyTorch.  \n",
        "In case of Linear Regression it is mean squared error.  \n",
        "So we can say as:  \n",
        "\n",
        "```\n",
        "criterion = nn.MSELoss()\n",
        "```\n",
        "So we stored it in criterion  \n",
        "Then we also set up the optimizer  \n",
        "  \n",
        "```\n",
        "optimizer = torch.optim.SGD()\n",
        "```\n",
        "**torch.optim.SGD**  \n",
        "torch.optim.SGD refers to the Stochastic Gradient Descent (SGD) optimizer. SGD is one of the simplest and most commonly used optimization algorithms in machine learning. The SGD optimizer updates the model's parameters using the gradients of the loss function with respect to each parameter  \n",
        "\n",
        "Our optimizer needs parameters that it should optimize.  \n",
        "So we can simply say:  \n",
        "\n",
        "**model.parameters()**:  \n",
        "model.parameters() is a method that returns an iterator over all the parameters of the model. These parameters are the ones that will be updated by the optimizer during the training process. When you pass model.parameters() to the optimizer, you are specifying that these are the parameters the optimizer should update.\n",
        "  \n",
        "**lr = learning_rate**:  \n",
        "lr stands for learning rate, which is a hyperparameter that controls the step size of the parameter updates during training. The learning rate determines how much the parameters are adjusted with respect to the gradient. A larger learning rate can lead to faster convergence but might overshoot the optimal solution, while a smaller learning rate might lead to more precise convergence but can take longer to train. The learning rate needs to be chosen carefully and is often determined through experimentation.\n",
        "```\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "```\n",
        "  \n",
        "**Training loop:**  \n",
        "Firstly we must define number of epochs, let's say we want to perform 100 training iterations.  \n",
        "\n",
        "\n",
        "```\n",
        "num_epochs = 100  \n",
        "for each epoch in range(num_epochs):\n",
        "```\n",
        "Here we do our steps in the training loop, the forward pass, the backward pass and the weight updates  \n",
        "  \n",
        "Let's suppose all about the forward pass and also the loss and then backward pass then updates.  \n",
        "  \n",
        "- So for forward pass we can say:  \n",
        "\n",
        "\n",
        "```\n",
        "y_predicted = model(x)\n",
        "```\n",
        "\n",
        "- Then we compute the loss by saying loss = criterion.  \n",
        "This needs the actual labels and predicted values: **loss = criterion(y_predicted, y)**  \n",
        "  \n",
        "Now, in the backward pass to calculate the gradients we just say,  \n",
        "So this will do the back propagation and calculate the gradients for us.  \n",
        "  \n",
        "- And then our update. Here we simply can say,  \n",
        "\n",
        "**optimizer.step()**  \n",
        "So this will update the weights and then before performing next iteration we have to be careful so we need to be empty our gradients now because whenever we all call the backward function, this will sum up the gradients into the .grad attribute. So now we want to empty this again  \n",
        "  \n",
        "- We simply say  \n",
        "optimizer.zero_grad()  \n",
        "Then we are done with the training loop.  \n",
        "  \n",
        "Then also print some information so let's say  \n",
        "```\n",
        "if (epoch + 1) % 10 == 0:\n",
        "```\n",
        "So every tenth iteration we want to print some information  \n",
        "```\n",
        "if (epoch + 1) % 10 == 0:\n",
        "    print(f'epoch: {epoch + 1}, loss = {loss.item():.4f}')\n",
        "```\n",
        ".4f -> only 4 decimal values.  \n",
        "  \n",
        "So now we are done and also we can plot this, To get all the predicted values.  \n",
        "  \n",
        "```\n",
        "predicted = model(x).detach().numpy()\n",
        "```\n",
        "  \n",
        "We call our final model 'model(x)' and with all the data now we need to convert in numpy again but before we do that we detach our tensor. So we want to prevent this operation from being tracked in our graph and our computational graph.  \n",
        "So this tensor has a required gradient arguments set to true, but now we want to be false. So this will generate a new tensor where our gradient calculation attribute is false.  \n",
        "And then we just call a numpy function to convert into numpy.\n",
        "  \n",
        "After this we are ready to plot the graph.  \n",
        "So first plot all our data **(x_numpy, y_numpy)** and if we want to plot this in particular colors lets say red dots we want for data points, so we can do as:  \n",
        "**(x_numpy, y_numpy, 'ro')**  \n",
        "  \n",
        "Then we can plot our generated or approximated functions (x_numpy, predicted, 'b') -> blue line  \n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4--LzKs04Mkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOGISTIC REGRESSION**"
      ],
      "metadata": {
        "id": "cavE4LBJrOZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare Data\n",
        "BC = datasets.load_breast_cancer()\n",
        "X, y = BC.data, BC.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale\n",
        "SC = StandardScaler()\n",
        "\n",
        "X_train = torch.from_numpy(SC.fit_transform(X_train).astype(np.float32))\n",
        "X_test = torch.from_numpy(SC.transform(X_test).astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "# Model\n",
        "# f = wx * b, sigmoid at the end\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "    def forward(self, x):\n",
        "        y_predicted = torch.sigmoid(self.linear(x))\n",
        "        return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# 2) Loss and Optimizer\n",
        "\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) Training Loop\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_predicted = model(X_train)\n",
        "    loss = criterion(y_predicted, y_train)\n",
        "\n",
        "    # Backward Pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Updates\n",
        "    optimizer.step()\n",
        "\n",
        "    # Zero Gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'epoch: {epoch + 1}, loss = {loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_predicted = model(X_test)\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(y_test).sum().item() / float(y_test.shape[0])\n",
        "    print(f'accuracy = {acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1TsPEY3Zre0A",
        "outputId": "3e5566c3-4b7d-4726-cc64-57bb6e3bc22f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 0.5600\n",
            "epoch: 20, loss = 0.4688\n",
            "epoch: 30, loss = 0.4093\n",
            "epoch: 40, loss = 0.3677\n",
            "epoch: 50, loss = 0.3369\n",
            "epoch: 60, loss = 0.3131\n",
            "epoch: 70, loss = 0.2942\n",
            "epoch: 80, loss = 0.2786\n",
            "epoch: 90, loss = 0.2655\n",
            "epoch: 100, loss = 0.2543\n",
            "accuracy = 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**    \n",
        "**Step 1: Prepare Data**  \n",
        "We first load the breast cancer dataset from sklearn and split it into training and testing sets. We then scale the features using StandardScaler to standardize the data.  \n",
        "\n",
        "```\n",
        "BC = datasets.load_breast_cancer()\n",
        "X, y = BC.data, BC.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "SC = StandardScaler()\n",
        "\n",
        "X_train = torch.from_numpy(SC.fit_transform(X_train).astype(np.float32))\n",
        "X_test = torch.from_numpy(SC.fit_transform(X_test).astype(np.float32))\n",
        "y_train = torch.from_numpy(SC.fit_transform(y_train.reshape(-1, 1)).astype(np.float32))\n",
        "y_test = torch.from_numpy(SC.fit_transform(y_test.reshape(-1, 1)).astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)\n",
        "```\n",
        "We use StandardScaler to scale the features. We reshape y_train and y_test to be column vectors before converting them to tensors. This is necessary because PyTorch expects the target to be a 2D tensor with one column.\n",
        "   \n",
        "**Step 2: Model**  \n",
        "We define a logistic regression model using torch.nn.Module. The model consists of a single linear layer followed by a sigmoid activation function.  \n",
        "\n",
        "```\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "```\n",
        "  \n",
        "**Step 3: Loss and Optimizer**  \n",
        "We use the binary cross-entropy loss function (nn.BCELoss) and the Stochastic Gradient Descent (SGD) optimizer.  \n",
        "\n",
        "\n",
        "```\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "```\n",
        "  \n",
        "**Step 4: Training Loop**  \n",
        "We train the model for 100 epochs. In each epoch, we perform the forward pass, compute the loss, perform the backward pass, update the model parameters, and zero the gradients.  \n",
        "\n",
        "\n",
        "```\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Forward pass and loss\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "\n",
        "  # Backward Pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Updates\n",
        "  optimizer.step()\n",
        "\n",
        "  # Zero Gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    print(f'epoch {epoch + 1}, loss = {loss.item():.4f}')\n",
        "```\n",
        "  \n",
        "**Step 5: Evaluation**  \n",
        "We evaluate the model on the test set by calculating the accuracy. We use torch.no_grad() to disable gradient calculation during evaluation.  \n",
        "\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "  acc = y_predicted_cls.eq(y_test).sum().item() / float(y_test.shape[0])\n",
        "  print(f'accuracy = {acc:.4f}')\n",
        "```"
      ],
      "metadata": {
        "id": "FfsVyvuKRTAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detailed Explanation:**  \n",
        "     \n",
        "**Data Preparation:**  \n",
        "- Load Data: We load the breast cancer dataset, which consists of features and target labels.  \n",
        "- Split Data:   \n",
        "We split the dataset into training and testing sets using train_test_split.\n",
        "- Standardize Features:  \n",
        "We scale the features to have zero mean and unit variance using StandardScaler.  \n",
        "  \n",
        "**Model Definition:**   \n",
        "- Linear Layer:  \n",
        "We define a single linear layer with n_input_features inputs and 1 output.  \n",
        "- Sigmoid Activation:  \n",
        "We apply the sigmoid activation function to the output of the linear layer to get the predicted probabilities.  \n",
        "  \n",
        "**Loss and Optimizer:**  \n",
        "- BCELoss: We use binary cross-entropy loss, which is suitable for binary classification problems.   \n",
        "- SGD Optimizer:  \n",
        "We use the Stochastic Gradient Descent optimizer to update the model parameters.\n",
        "  \n",
        "**Training Loop:**  \n",
        "- Forward Pass:   \n",
        "We pass the training data through the model to get the predicted values.  \n",
        "- Compute Loss:  \n",
        "We compute the loss between the predicted and actual values.  \n",
        "- Backward Pass:  \n",
        "We perform backpropagation to compute the gradients of the loss with respect to the model parameters.   \n",
        "- Update Parameters:  \n",
        "We update the model parameters using the optimizer.  \n",
        "- Zero Gradients:  \n",
        "We zero the gradients to prevent accumulation in the next iteration.  \n",
        "  \n",
        "**Evaluation:**  \n",
        "- Predict on Test Set:   \n",
        "We pass the test data through the model to get the predicted values.  \n",
        "- Calculate Accuracy:  \n",
        "We round the predicted probabilities to get the predicted classes and compute the accuracy by comparing with the actual labels."
      ],
      "metadata": {
        "id": "j9E3NRoMSyc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression Model:\n",
        "# The logistic regression model predicts the probability that a given input sample belongs to a certain class.\n",
        "# It uses a linear function followed by a sigmoid activation function:\n",
        "\n",
        "# Sigmoid(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "# Loss Function:\n",
        "# The binary cross-entropy (BCE) loss function measures the difference between the predicted probabilities\n",
        "# and the actual binary labels (0 or 1).\n",
        "\n",
        "# BCELoss = -1/N * Σ [ y_i * log(ŷ_i) + (1 - y_i) * log(1 - ŷ_i) ]\n",
        "\n",
        "# Optimization:\n",
        "# We use stochastic gradient descent (SGD) to optimize the model parameters (weights and bias).\n",
        "\n",
        "# Training Loop:\n",
        "# In each epoch, we calculate the loss for the training set, perform backpropagation to compute the gradients,\n",
        "# update the parameters using SGD, and zero the gradients.\n",
        "\n",
        "# Evaluation:\n",
        "# After training, we use the trained model to predict the classes for the test set.\n",
        "# We round the predicted probabilities to the nearest integer (0 or 1) and calculate the accuracy by comparing with the actual labels."
      ],
      "metadata": {
        "id": "Olvr5pcGUwBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Model:**\n",
        "The logistic regression model predicts the probability that a given input sample belongs to a certain class. It uses a linear function followed by a sigmoid activation function:\n",
        "\n",
        "**Sigmoid(𝑧) = 1 / (1 + (e)^ -𝑧)**  \n",
        "where 𝑧 = 𝑤1 𝑥1 + 𝑤2 𝑥2 + … + 𝑤𝑛 𝑥𝑛 + 𝑏 is the linear combination of input features (𝑥𝑖) weighted by coefficients (𝑤𝑖) plus a bias term (𝑏). The sigmoid function maps the output to a value between 0 and 1, representing the probability of the positive class.  \n",
        "  \n",
        "**Loss Function:**  \n",
        "The binary cross-entropy (BCE) loss function measures the difference between the predicted probabilities and the actual binary labels (0 or 1). It is defined as:  \n",
        "**BCELoss= - [1/n] ∑ [ yi log(ŷi) + (1−yi) log(1−ŷi) ]**  \n",
        "where 𝑁 is the number of samples, 𝑦𝑖 is the actual label (0 or 1) for sample 𝑖, and ŷi is the predicted probability for sample 𝑖.  \n",
        "  \n",
        "**Optimization:**\n",
        "We use stochastic gradient descent (SGD) to optimize the model parameters (weights and bias) based on the gradients of the loss function with respect to these parameters. The learning rate (𝛼) controls the size of the steps taken in the direction of the gradients:  \n",
        "**wi = wi − α (∂BCELoss / ∂wi)**  \n",
        "\n",
        "**b = b − α (∂BCELoss / ∂b)**  \n",
        "  \n",
        "**Training Loop:**  \n",
        "In each epoch, we calculate the loss for the training set, perform backpropagation to compute the gradients, update the parameters using SGD, and zero the gradients. We also print the loss every 10 epochs to monitor the training progress.\n",
        "  \n",
        "**Evaluation:**  \n",
        "After training, we use the trained model to predict the classes for the test set. We round the predicted probabilities to the nearest integer (0 or 1) and calculate the accuracy by comparing with the actual labels.   \n",
        "  \n",
        "In the logistic regression code, we perform the following steps:  \n",
        "  \n",
        "- Prepare Data: We load the breast cancer dataset and split it into training and testing sets. We also scale the features using StandardScaler.  \n",
        "  \n",
        "- Model: We define a logistic regression model using a linear layer followed by a sigmoid activation function.  \n",
        "    \n",
        "- Loss and Optimizer: We use binary cross-entropy loss and the SGD optimizer.  \n",
        "  \n",
        "- Training Loop: We train the model for 100 epochs. In each epoch, we perform a forward pass, compute the loss, perform a backward pass, update the parameters, and zero the gradients.  \n",
        "    \n",
        "- Evaluation: We evaluate the model on the test set and calculate the accuracy.  \n"
      ],
      "metadata": {
        "id": "Q2Y4CH4dU6pG"
      }
    }
  ]
}